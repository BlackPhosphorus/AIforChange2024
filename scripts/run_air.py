{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8814120,"sourceType":"datasetVersion","datasetId":5301969},{"sourceId":8821683,"sourceType":"datasetVersion","datasetId":5307105}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:19:26.360252Z\",\"iopub.execute_input\":\"2024-06-30T04:19:26.361276Z\",\"iopub.status.idle\":\"2024-06-30T04:19:26.370575Z\",\"shell.execute_reply.started\":\"2024-06-30T04:19:26.361229Z\",\"shell.execute_reply\":\"2024-06-30T04:19:26.369655Z\"}}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nfrom scipy.io import netcdf\nfrom scipy import stats\nimport random\nimport os\nimport torch\nfrom statsmodels.tsa.statespace.varmax import VARMAX\nimport tensorflow as tf\nfrom torch.utils.data import Dataset\nfrom transformers import PatchTSTConfig, PatchTSTForPrediction, AutoformerModel, AutoformerConfig, AutoformerForPrediction, TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction, InformerConfig, InformerForPrediction, is_torch_xla_available\nfrom transformers.optimization import Adafactor, get_cosine_schedule_with_warmup, AdamW\nfrom transformers import Trainer, TrainingArguments\nimport gc\nimport time\nimport logging\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# Set logging level to error\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:14:26.793268Z\",\"iopub.execute_input\":\"2024-06-30T04:14:26.794161Z\",\"iopub.status.idle\":\"2024-06-30T04:14:26.799170Z\",\"shell.execute_reply.started\":\"2024-06-30T04:14:26.794130Z\",\"shell.execute_reply\":\"2024-06-30T04:14:26.798221Z\"}}\nclass MainConfig:\n    context_length = 10\n    lags_sequence = [1, 2, 3, 4, 5, 6, 7]\n    prediction_length = 14\n    wandb = False\n    train_batch_size = 4\n    valid_batch_size = 8\n    lr = 1e-4\n    epochs = 20\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:44:21.976118Z\",\"iopub.execute_input\":\"2024-06-30T04:44:21.976502Z\",\"iopub.status.idle\":\"2024-06-30T04:44:21.981045Z\",\"shell.execute_reply.started\":\"2024-06-30T04:44:21.976470Z\",\"shell.execute_reply\":\"2024-06-30T04:44:21.979911Z\"}}\nYEARS = list(range(2000, 2025))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:44:22.091887Z\",\"iopub.execute_input\":\"2024-06-30T04:44:22.092253Z\",\"iopub.status.idle\":\"2024-06-30T04:44:22.098312Z\",\"shell.execute_reply.started\":\"2024-06-30T04:44:22.092221Z\",\"shell.execute_reply\":\"2024-06-30T04:44:22.097344Z\"}}\nif MainConfig.wandb:\n    import wandb\n    os.environ[\"WANDB_DISABLED\"] = \"false\"\n    os.environ[\"WANDB_RUN_GROUP\"] = \"experiment-\" + wandb.util.generate_id()\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n    wandb.init(\n        project=\"science-llm-mc\",\n        group=\"experiment-2\",\n        job_type=f\"fold_{fold}\"\n    )\nelse:\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:44:22.288032Z\",\"iopub.execute_input\":\"2024-06-30T04:44:22.288877Z\",\"iopub.status.idle\":\"2024-06-30T04:44:22.295076Z\",\"shell.execute_reply.started\":\"2024-06-30T04:44:22.288846Z\",\"shell.execute_reply\":\"2024-06-30T04:44:22.294057Z\"}}\ndef seedBasic(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \ndef seedTorch(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n      \ndef seedEverything(seed):\n    seedBasic(seed)\n    seedTorch(seed)\n    \nseedEverything(2007)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:45:04.419992Z\",\"iopub.execute_input\":\"2024-06-30T04:45:04.420380Z\",\"iopub.status.idle\":\"2024-06-30T04:45:04.428101Z\",\"shell.execute_reply.started\":\"2024-06-30T04:45:04.420345Z\",\"shell.execute_reply\":\"2024-06-30T04:45:04.427352Z\"}}\ndef build_dataset():\n    data = []\n    for year in YEARS:\n        file2read = netcdf.NetCDFFile(f\"/kaggle/input/airdataset/air.{year}.nc\",'r')\n        variables = file2read.variables.keys()\n        data.append({k : file2read.variables[k][:]*1 for k in variables})\n        data[-1][\"air\"] = np.abs(data[-1][\"air\"])\n        \n    return data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:46:22.108402Z\",\"iopub.execute_input\":\"2024-06-30T04:46:22.109231Z\",\"iopub.status.idle\":\"2024-06-30T04:46:22.114771Z\",\"shell.execute_reply.started\":\"2024-06-30T04:46:22.109198Z\",\"shell.execute_reply\":\"2024-06-30T04:46:22.113767Z\"}}\ndef process_location(location, data):\n    dataset = []\n    \n    for j, yearly_data in enumerate(data):\n        dataset.append(yearly_data[\"air\"][:,:,location[0], location[1]])\n        \n    dataset = np.concatenate(dataset, 0)\n    \n #   dataset = pd.DataFrame(dataset / 10000)\n    \n#    dataset.columns = [f\"Level {i+1}\" for i in range(12)]\n    \n    return dataset, get_dates(len(dataset))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:46:22.316844Z\",\"iopub.execute_input\":\"2024-06-30T04:46:22.317201Z\",\"iopub.status.idle\":\"2024-06-30T04:46:22.322669Z\",\"shell.execute_reply.started\":\"2024-06-30T04:46:22.317171Z\",\"shell.execute_reply\":\"2024-06-30T04:46:22.321674Z\"}}\ndef get_dates(n):\n    today = datetime.now()\n\n    dates_list = [(today - timedelta(days=i-1)) for i in range(n)]\n    dates_processed = [[date.month, date.day, date.year] for date in dates_list]\n    return np.array(dates_processed[::-1])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:46:22.477271Z\",\"iopub.execute_input\":\"2024-06-30T04:46:22.478025Z\",\"iopub.status.idle\":\"2024-06-30T04:46:22.483204Z\",\"shell.execute_reply.started\":\"2024-06-30T04:46:22.477993Z\",\"shell.execute_reply\":\"2024-06-30T04:46:22.482335Z\"}}\ndef get_dates_ahead(n):\n    today = datetime.now()\n\n    dates_list = [(today + timedelta(days=i)) for i in range(n)]\n    dates_processed = [[date.month, date.day, date.year] for date in dates_list]\n    return np.array(dates_processed)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:46:22.652863Z\",\"iopub.execute_input\":\"2024-06-30T04:46:22.653715Z\",\"iopub.status.idle\":\"2024-06-30T04:46:22.659386Z\",\"shell.execute_reply.started\":\"2024-06-30T04:46:22.653682Z\",\"shell.execute_reply\":\"2024-06-30T04:46:22.658362Z\"}}\nclass WeatherDataset(Dataset):\n    def __init__(self, dict_values):\n        self.dict_values = dict_values\n        \n    def __len__(self):\n        return len(self.dict_values[\"past_values\"])\n    \n    def __getitem__(self, idx):\n        return {\n            k : torch.tensor(v[idx]).float() for k, v in self.dict_values.items() \n        }\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:46:22.984706Z\",\"iopub.execute_input\":\"2024-06-30T04:46:22.985063Z\",\"iopub.status.idle\":\"2024-06-30T04:46:23.005569Z\",\"shell.execute_reply.started\":\"2024-06-30T04:46:22.985034Z\",\"shell.execute_reply\":\"2024-06-30T04:46:23.004656Z\"}}\ndef run(location_idxs, data):\n        \n    processed_data, dates = process_location(location_idxs, data)\n        \n    dict_vals = {\n        \"past_values\" : [],\n        \"past_time_features\" : [],\n        \"past_observed_mask\" : [],\n        \"future_values\" : [],\n        \"future_time_features\" : [],\n    }\n\n    for i in range(0, len(processed_data) - MainConfig.context_length - MainConfig.prediction_length - max(MainConfig.lags_sequence), MainConfig.context_length + MainConfig.prediction_length + max(MainConfig.lags_sequence)):\n        dict_vals[\"past_values\"].append(processed_data[i : i + MainConfig.context_length + max(MainConfig.lags_sequence)])\n        dict_vals[\"past_time_features\"].append(dates[i : i + MainConfig.context_length + max(MainConfig.lags_sequence)])\n        dict_vals[\"past_observed_mask\"].append(np.ones((MainConfig.context_length + max(MainConfig.lags_sequence), 12)))\n\n        dict_vals[\"future_values\"].append(processed_data[i + MainConfig.context_length + max(MainConfig.lags_sequence) : i + MainConfig.context_length + max(MainConfig.lags_sequence) + MainConfig.prediction_length])\n        dict_vals[\"future_time_features\"].append(dates[i + MainConfig.context_length + max(MainConfig.lags_sequence) : i + MainConfig.context_length + max(MainConfig.lags_sequence) + MainConfig.prediction_length])\n        \n    train_ds = WeatherDataset(dict_vals)\n        \n    config = TimeSeriesTransformerConfig(\n                          prediction_length=MainConfig.prediction_length, \n                          context_length=MainConfig.context_length,\n                          lags_sequence=MainConfig.lags_sequence,\n                          num_time_features=3,                              \n                          d_model=128,\n                          encoder_layers=2,\n                          decoder_layers=2,\n                          encoder_ffn_dim=64,\n                          decoder_ffn_dim=64,\n                          distribution_output=\"student_t\",\n                          moving_average=35,\n                          num_parallel_samples=100,\n                          input_size=12,                \n        )\n    model = TimeSeriesTransformerForPrediction(config).to(MainConfig.device)\n    \n    optimizer = AdamW(model.parameters(), lr=MainConfig.lr)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=200, num_training_steps=int(len(train_ds)/MainConfig.train_batch_size)*MainConfig.epochs)\n\n    arguments = TrainingArguments(output_dir=\"/kaggle/working/\", \n                          learning_rate=MainConfig.lr, \n                          per_device_train_batch_size=MainConfig.train_batch_size, \n                          per_device_eval_batch_size=MainConfig.valid_batch_size, \n                          save_strategy=\"no\",\n                          num_train_epochs=10,\n                          report_to=\"wandb\" if MainConfig.wandb else None,\n                          ignore_data_skip=True,\n                          warmup_ratio=0.8,\n                          label_smoothing_factor=0.0,\n                          logging_strategy='no',\n                          )\n\n    trainer = Trainer(model=model, \n             train_dataset=train_ds, \n             args=arguments,\n             optimizers=(optimizer, scheduler)\n             )\n\n    trainer.train()\n    \n    past_values = processed_data[-(MainConfig.context_length + max(MainConfig.lags_sequence)):]\n    past_time_features = dates[-(MainConfig.context_length + max(MainConfig.lags_sequence)):]\n    past_observed_mask = np.ones((MainConfig.context_length + max(MainConfig.lags_sequence), 12))\n\n    model.eval()\n    \n    sample = {\n        \"past_values\" : past_values,\n        \"past_time_features\" : past_time_features,\n        \"past_observed_mask\" : past_observed_mask,\n        \"future_time_features\" : get_dates_ahead(100),\n    }\n\n    sample = {k : torch.tensor(v).unsqueeze(0).float().to(MainConfig.device) for k, v in sample.items()}\n    \n    outputs = model.generate(\n            past_values = sample[\"past_values\"],\n            past_time_features = sample[\"past_time_features\"],\n            past_observed_mask = sample[\"past_observed_mask\"],\n            future_time_features = sample[\"future_time_features\"],\n        )\n    \n    preds = stats.trim_mean(outputs.sequences[0].cpu().detach().numpy(), 0.25, axis=0)\n    \n    del model, trainer, optimizer, scheduler, dict_vals\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return np.asarray(preds * -10000, dtype=\"int\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:46:23.323885Z\",\"iopub.execute_input\":\"2024-06-30T04:46:23.324610Z\",\"iopub.status.idle\":\"2024-06-30T04:46:25.008674Z\",\"shell.execute_reply.started\":\"2024-06-30T04:46:23.324572Z\",\"shell.execute_reply\":\"2024-06-30T04:46:25.007790Z\"}}\ndata = build_dataset()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:48:06.996221Z\",\"iopub.execute_input\":\"2024-06-30T04:48:06.996605Z\",\"iopub.status.idle\":\"2024-06-30T04:48:07.001846Z\",\"shell.execute_reply.started\":\"2024-06-30T04:48:06.996573Z\",\"shell.execute_reply\":\"2024-06-30T04:48:07.000937Z\"}}\nlat = [data[0][\"lat\"][0:34][i] for i in range(34) if i % 3 == 0]\nlon = [data[0][\"lon\"][10:70][i] for i in range(60) if i % 3 == 0]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:48:12.716320Z\",\"iopub.execute_input\":\"2024-06-30T04:48:12.716695Z\",\"iopub.status.idle\":\"2024-06-30T04:48:12.724195Z\",\"shell.execute_reply.started\":\"2024-06-30T04:48:12.716666Z\",\"shell.execute_reply\":\"2024-06-30T04:48:12.723328Z\"}}\ndef MultiExponentialSmoothing(data):\n    results = []\n    for level in range(12):\n        model = ExponentialSmoothing(processed_data[:,level], seasonal='add', seasonal_periods=365)\n        model_fit = model.fit()\n        n_forecast = 365\n        forecast = model_fit.forecast(steps=n_forecast)\n        results.append(forecast.reshape(365, 1))\n        \n    return np.concatenate(results, 1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:48:12.992282Z\",\"iopub.execute_input\":\"2024-06-30T04:48:12.993228Z\",\"iopub.status.idle\":\"2024-06-30T04:50:39.433026Z\",\"shell.execute_reply.started\":\"2024-06-30T04:48:12.993186Z\",\"shell.execute_reply\":\"2024-06-30T04:50:39.431619Z\"}}\ncur = 0\ntotal = len(lat) * len(lon)\n\nresults = {}\n\nfor i in range(0, len(lat)):\n    for j in range(0, len(lon)):\n        start_time = time.time()\n\n        processed_data, dates = process_location((i, j), data)\n        \n        output = MultiExponentialSmoothing(processed_data)\n        \n        results[f\"{lat[i]}_{lon[j]}\"] = output\n        \n        cur += 1\n\n        print(f\"{cur} / {total}\")\n        \n        end_time = time.time()\n        \n        elapsed_time = end_time - start_time\n        print(f'Elapsed time: {elapsed_time} seconds')\n        print(f'Estimated Time: {(total - cur) * elapsed_time} seconds')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-30T04:46:36.456448Z\",\"iopub.execute_input\":\"2024-06-30T04:46:36.457060Z\",\"iopub.status.idle\":\"2024-06-30T04:46:36.475168Z\",\"shell.execute_reply.started\":\"2024-06-30T04:46:36.457030Z\",\"shell.execute_reply\":\"2024-06-30T04:46:36.474246Z\"}}\nimport json\n\nclass NpEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return super(NpEncoder, self).default(obj)\n\nwith open('air_temps.json', 'w') as f:\n    json.dump(results, f, cls=NpEncoder)\n\n# %% [code]\n","metadata":{"_uuid":"9ff1ff64-e13f-426e-9981-e281c310ecf9","_cell_guid":"9f59c020-5023-437c-9ece-e2ae7dc9ade6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}